<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="google-site-verification" content="RbzwkvoL0jexA8TMZ7m0Lva6O-PvrL_XO9veOpXzc_E" />
    <title>Hyeonbin Hwang</title>
    <link rel="stylesheet" href="./styles.css">
</head>

<body>
    <div class="intro">
        <div class="intro-text">
            <h3>안녕하세요!</h3>
            <h1>I'm <span class="highlight">Hyeonbin</span> ദ്ദി・ᴗ・)✧ </h1>
            <h2>#NLProc</h2>
            <ul class="contact-info">
                <li>Email: <a href="mailto:hbin0701@kaist.ac.kr">hbin0701@kaist.ac.kr</a></li>
                <li>Location: Seoul, South Korea</li>
            </ul>
            <div class="social-media">
                <a href="https://github.com/hbin0701" class="social-media-item" target="_blank" aria-label="GitHub">
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                        <path
                            d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.230 3.297-1.230.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z" />
                    </svg>
                </a>
                <a href="https://scholar.google.com/citations?user=RoEV6T0AAAAJ" class="social-media-item"
                    target="_blank" aria-label="Google Scholar">
                    <svg xmlns="http://www.w3.org/2000/svg" aria-label="Google Scholar" role="img" viewBox="0 0 512 512"
                        width="24" height="24">
                        <rect width="512" height="512" fill="#4285f4" />
                        <path fill="#ffffff"
                            d="M213 111l-107 94h69c5 45 41 64 78 67-7 18-4 27 7 39-43 1-103 26-103 67 4 45 63 54 92 54 38 1 81-19 90-54 4-35-10-54-31-71-23-18-28-28-21-40 15-17 35-27 39-51 2-17-2-28-6-43l45-38-1 16c-3 2-5 6-5 9v103c2 13 22 11 23 0V160c0-3-2-7-5-8v-25l16-16zm58 141c-61 10-87-87-38-99 56-11 83 86 38 99zm-5 73c60 13 61 63 10 78-44 9-82-4-81-30 0-25 35-48 71-48z" />
                    </svg>
                </a>
                <a href="https://www.linkedin.com/in/hyeonbin-hwang-1b8a9222b/" class="no-underline" target="_blank">
                    <svg height="24" , width="24" , viewBox="0 0 72 72" , xmlns="http://www.w3.org/2000/svg">
                        <g fill="none" fill-rule="evenodd">
                            <path
                                d="M8,72 L64,72 C68.418278,72 72,68.418278 72,64 L72,8 C72,3.581722 68.418278,-8.11624501e-16 64,0 L8,0 C3.581722,8.11624501e-16 -5.41083001e-16,3.581722 0,8 L0,64 C5.41083001e-16,68.418278 3.581722,72 8,72 Z"
                                fill="#007EBB" />
                            <path
                                d="M62,62 L51.315625,62 L51.315625,43.8021149 C51.315625,38.8127542 49.4197917,36.0245323 45.4707031,36.0245323 C41.1746094,36.0245323 38.9300781,38.9261103 38.9300781,43.8021149 L38.9300781,62 L28.6333333,62 L28.6333333,27.3333333 L38.9300781,27.3333333 L38.9300781,32.0029283 C38.9300781,32.0029283 42.0260417,26.2742151 49.3825521,26.2742151 C56.7356771,26.2742151 62,30.7644705 62,40.051212 L62,62 Z M16.349349,22.7940133 C12.8420573,22.7940133 10,19.9296567 10,16.3970067 C10,12.8643566 12.8420573,10 16.349349,10 C19.8566406,10 22.6970052,12.8643566 22.6970052,16.3970067 C22.6970052,19.9296567 19.8566406,22.7940133 16.349349,22.7940133 Z M11.0325521,62 L21.769401,62 L21.769401,27.3333333 L11.0325521,27.3333333 L11.0325521,62 Z"
                                fill="#FFF" />
                        </g>
                    </svg>
                </a>
            </div>
        </div>

        <div class="profile-picture">
            <img src="./IMG_5418.jpg" alt="Hyeonbin">
        </div>
    </div>

    <div id="about" class="about-container">
        <div class="about hidden">
            <div class="about-img">
                <img src="./IMG_2070.JPG" alt="IMG_2070" class="img-hover">
            </div>

            <div class="about-text">
                <h3>About Me</h3>
                I am a second year M.S. student at KAIST AI, in <a class="no-underline"
                    href="https://lklab.kaist.ac.kr/">Language & Knowledge Lab.</a>
                <br /><br />
                I study how Large Language Models (LLMs) can improve their reasoning
                abilities using RL-based approaches. I am also interested in understanding the mechanisms behind the
                development of such reasoning capabilities. <br /><br />
                In my free time, I enjoy playing Jazz and Soccer.
            </div>
        </div>
    </div>


    <div id="experiences" class="experiences-container hidden">
        <div class="education">
            <h3>Education</h3>
            <div class="edu-box">
                <div class="edu-degree">
                    <h4>Master's Degree</h4>
                    <h5>KAIST AI <br /><span>Advisor: Minjoon Seo</span></h5>
                </div>
                <div class="date">2024.02~</div>
            </div>
            <div class="edu-box">
                <div class="edu-degree">
                    <h4>Bachelor's Degree</h4>
                    <h5>KAIST, <span class="comp">School of Computing</span><br /><span>Grade: 3.96 / 4.3</span></h5>
                </div>
                <div class="date">2019.08 ~ 2024.02</div>
            </div>
        </div>
        <div class="work-experiences">
            <h3>Reserach Internships</h3>
            <div class="exp-box">
                <h5>LK Lab, KAIST <br>
                    <span>on Large Language Model's prompting space</span><br />
                    <span class="date">2022.12 ~ 2024.02</span>
                </h5>
            </div>
            <div class="exp-box">
                <h5>Naver CLOVA, Healthcare AI <br>
                    <span>on Electronic Health Record (EHR) Summarization and Embedding</span>
                    <span class="date">2022.03 ~ 2022.08</span>
                </h5>
            </div>
            <div class="exp-box">
                <h5>KAIST Computer Vision & Learning Lab <br>
                    <span>on 2D Image Detection and Classification</span><br />
                    <span class="date">2021.06 ~ 2021.09</span>
                </h5>
            </div>
            <div class="exp-box">
                <h5>w/ Acryl Inc. <br>
                    <span>on Facial Expression Recognition</span><br />
                    <span class="date">2021.03 ~ 2021.09</span>
                </h5>
            </div>
        </div>
    </div>

    <div id="publications" class="publication-box hidden">
        <h3>Selected Publications</h3>

        <div class="publication-row">
            <div class="publication-year">2025</div>
            <div class="publications">
                <div class="card">
                    <div class="card-inner">
                        <div class="card-front">
                            <h5>The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models
                                with Language Models</h5>
                            <h6 class="authors">S Kim, J Suk, ... (others not shown), <span class="highlight-author">H
                                    Hwang</span>, ...</h6>
                            <h6>
                                <a href="https://arxiv.org/abs/2406.05761">PAPER</a>
                                <a
                                    href="https://github.com/prometheus-eval/prometheus-eval/tree/main/BiGGen-Bench">CODE</a>
                            </h6>
                            <h6 style="margin-top: -15px; font-weight: bold"><i>@ NAACL 2025</i></h6>
                        </div>
                        <div class="card-back">
                            <p>We introduce <b>BiGGen Bench</b>, a comprehensive benchmark that evaluates nine distinct
                                capabilities of language models across 77 tasks using instance-specific criteria,
                                addressing limitations of abstract and biased evaluation methods while demonstrating its
                                utility on 103 frontier LMs.</p>
                            <p>#Evaluation #Benchmark</p>
                        </div>
                    </div>
                </div>

                <div class="card">
                    <div class="card-inner">
                        <div class="card-front">
                            <h5>Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge
                                Acquisition</h5>
                            <h6 class="authors">J Kim, H Lee, H Cho, J Jang, <span class="highlight-author">H
                                    Hwang</span>, S Won, Y Ahn, D Lee, M Seo</h6>
                            <h6>
                                <a href="https://arxiv.org/abs/2410.01380">PAPER</a>
                                <a href="https://github.com/kaistAI/Knowledge-Entropy">CODE</a>
                            </h6>
                            <h6 style="margin-top: -15px; font-weight: bold"><i>@ ICLR 2025 & AAAI 2025 KnowFM WS
                                    (Oral)</i></h6>
                        </div>
                        <div class="card-back">
                            <p>
                                We introduce the concept of <b>knowledge entropy</b> to analyze how a model's reliance
                                on
                                memory
                                sources evolves during pretraining, revealing that decreasing knowledge entropy impairs
                                knowledge acquisition and retention, while reactivating inactive memory sources can
                                somewhat alleviate this issue.</p>
                            </h5>
                            <p>#Knowledge Entropy #Pretraining</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="publication-row">
            <div class="publication-year">2024</div>
            <div class="publications">
                <div class="card">
                    <div class="card-inner">
                        <div class="card-front">
                            <h5>Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models
                                with Fine-grained Rewards</h5>
                            <h6 class="authors"><span class="highlight-author">H Hwang</span>, D Kim, S Kim, S Ye, M Seo
                            </h6>
                            <h6>
                                <a href="https://arxiv.org/abs/2404.10346">PAPER</a>
                                <a href="https://github.com/hbin0701/Self-Explore">CODE</a>
                            </h6>
                            <h6 style="margin-top: -15px; font-weight: bold"><i>@ EMNLP 2024 (Findings) & ACL 2024 NLRSE
                                    WS (Oral)</i></h6>
                        </div>
                        <div class="card-back">
                            <p>We propose <b>Self-Explore</b>, a method where LLMs identify and learn from their first
                                reasoning errors to self-improve using fine-grained rewards, achieving notable
                                performance gains over supervised fine-tuning on reasoning tasks like GSM8K and MATH.
                            </p>
                            <p>#Reasoning #Self-Training #Fine-grained Rewards</p>
                        </div>
                    </div>
                </div>

                <div class="card">
                    <div class="card-inner">
                        <div class="card-front">
                            <h5>Flask: Fine-grained Language Model Evaluation Based on Alignment Skill Sets</h5>
                            <h6 class="authors">S Ye, D Kim, S Kim, <span class="highlight-author">H Hwang</span>, S
                                Kim, Y Jo, J Thorne, J Kim, M Seo</h6>
                            <h6>
                                <a href="https://arxiv.org/abs/2307.10928">PAPER</a>
                                <a href="https://github.com/kaistAI/FLASK">CODE</a>
                            </h6>
                            <h6 style="margin-top: -15px; font-weight: bold"><i>@ ICLR 2024 (Spotlight)</i></h6>
                        </div>
                        <div class="card-back">
                            <p>We introduce <b>FLASK</b>, a fine-grained evaluation protocol that decomposes
                                instruction-following into skill set-level scoring, offering a more interpretable and
                                reliable assessment of LLM performance compared to coarse-grained methods.</p>
                            <p>#Evaluation #Fine-grained </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="publication-row">
            <div class="publication-year" style="visibility: hidden" : white>2024</div>
            <div class="publications">
                <div class="card">
                    <div class="card-inner">
                        <div class="card-front">
                            <h5>Investigating the Effectiveness of Task-Agnostic <br> Prefix Prompt for Instruction
                                Following
                            </h5>
                            <h6 class="authors">S Ye, <span class="highlight-author">H Hwang</span>, S Yang, H Yun, Y
                                Kim, M Seo</h6>
                            <h6>
                                <a href="https://arxiv.org/abs/2302.14691">PAPER</a>
                                <a href="https://github.com/seonghyeonye/TAPP">CODE</a>
                            </h6>
                            <h6 style="margin-top: -15px; font-weight: bold"><i>@ AAAI 2024</i></h6>
                        </div>
                        <div class="card-back">
                            <p>
                                We demonstrate that prepending a <b>Task-Agnostic Prefix Prompt</b> (TAPP) to inputs
                                significantly enhances the instruction-following ability of both pre-trained and
                                instruction-tuned LLMs during inference, suggesting that a fixed, heuristic-based prompt
                                can activate underutilized capabilities in these models.</p>
                            <p>#Instruction Following #Prompt</p>
                        </div>
                    </div>

                </div>
                <div class="card" style="visibility: hidden">
                </div>
            </div>
        </div>

        <div class="publication-row">
            <div class="publication-year">2022</div>
            <div class="publications">
                <div class="card">
                    <div class="card-inner">
                        <div class="card-front">
                            <h5>MED-SE: Medical Entity Definition-based Sentence Embedding</h5>
                            <h6 class="authors"><span class="highlight-author">H Hwang</span>, H Yoo, Y Choi</h6>
                            <h6>
                                <a href="https://arxiv.org/abs/2212.04734">PAPER</a>
                            </h6>
                            <h6 style="margin-top: -15px; font-weight: bold"><i>Preprint (Internship Work @ Naver
                                    Healthcare AI)</i></h6>
                        </div>
                        <div class="card-back">
                            <p> <b>MED-SE</b> framework leverages medical entity definitions
                                to achieve superior sentence embedding performance for clinical texts,
                                highlighting the effectiveness of entity-centric contrastive learning
                                in addressing domain-specific challenges.</p>
                            <p>#Healthcare #EHR #Sentence Embedding</p>
                        </div>
                    </div>
                </div>

                <div class="card">
                    <div class="card-inner">
                        <div class="card-front">
                            <h5>Vision Transformer Equipped with Neural Resizer on Facial Expression Recognition Task
                            </h5>
                            <h6 class="authors"><span class="highlight-author">H Hwang</span>, S Kim, WJ Park, J Seo, K
                                Ko, H Yeo</h6>
                            <h6>
                                <a href="https://arxiv.org/abs/2204.02181">PAPER</a>
                                <a href="https://github.com/hbin0701/VT_with_NR_for_FER">CODE</a>
                            </h6>
                            <h6 style="margin-top: -15px; font-weight: bold"><i>@ ICASSP 2022</i></h6>
                        </div>
                        <div class="card-back">
                            <p><b>Neural Resizer</b> framework enhances Transformer performance in facial
                                expression recognition by compensating for low-resolution data through a data-driven
                                resizing method and a novel F-PDLS loss function, achieving near state-of-the-art
                                results.</p>
                            <p>#Computer Vision #Facial Expression Recognition</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <footer>
        @hbin0701, Updated January 25th, 2025
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function () {
            // Select all cards
            const cards = document.querySelectorAll('.card');

            // Add click event listener to each card
            cards.forEach(card => {
                card.addEventListener('click', function () {
                    card.classList.toggle('flipped'); // Toggle the flipped class
                });
            });
        });

        document.addEventListener("DOMContentLoaded", function () {
            // Get the sections to observe
            const introSection = document.querySelector('.intro');
            const aboutSection = document.querySelector('.about');
            const expSection = document.querySelector('.experiences-container');
            const publicationBox = document.querySelector('.publication-box');

            // Create an Intersection Observer
            const observer = new IntersectionObserver(entries => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        // Add the 'visible' class to the section
                        entry.target.classList.add('visible');
                    } else {
                        // Remove the 'visible' class when it's out of view
                        entry.target.classList.remove('visible');
                    }
                });
            }, {
                threshold: 0.2 // Adjust this value if needed
            });

            // Observe the sections
            if (introSection) {
                observer.observe(introSection);
            }
            if (aboutSection) {
                observer.observe(aboutSection);
            }
            if (expSection) {
                observer.observe(expSection);
            }
            if (publicationBox) {
                observer.observe(publicationBox);
            }
        });

    </script>

</body>


</html>
